import aiohttp
from bs4 import BeautifulSoup
from config import URL

async def fetch_latest_news():
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–æ–≤–æ—Å—Ç—å —Å —Å–∞–π—Ç–∞"""
    async with aiohttp.ClientSession() as session:
        async with session.get(URL) as response:
            html = await response.text()

    soup = BeautifulSoup(html, "html.parser")
    news_section = soup.find("div", class_="news-content")  # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏

    if not news_section:
        return "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    # –ù–∞–π—Ç–∏ –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
    news_items = news_section.find_all(["h1", "h2", "p", "a"])

    if not news_items:
        return "–ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å
    latest_news = ""
    first_news = news_items[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤

    base_url = "https://it.tlscontact.com"  # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π URL –¥–ª—è —Å—Å—ã–ª–æ–∫
    for element in first_news:
        if element.name in ["h1", "h2"]:
            latest_news += f"üì∞ *{element.get_text()}*\n\n"
        elif element.name == "p":
            latest_news += f"{element.get_text()}\n\n"
        elif element.name == "a":
            link = element.get("href")
            full_link = f"{base_url}{link}" if link.startswith("/") else link
            latest_news += f"[üîó –ü–æ–¥—Ä–æ–±–Ω–µ–µ]({full_link})\n\n"

    return latest_news
